{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import time\n",
    "\n",
    "def samedate(hotspot_date, climate_date):\n",
    "    # this function checks if two dates are the same\n",
    "    # This function was made because date is forrmatted differently in hotspot_historic and climate_historic\n",
    "    # eg. hotspot historic '27/12/2018' and climate historic '27/12/18'\n",
    "    actual_date = []\n",
    "    final_date = []\n",
    "    join_string = ''\n",
    "    date_length = len(hotspot_date)\n",
    "    for k in range(date_length - 1, -1, -1):\n",
    "        if k == 7 or k == 6:\n",
    "            continue\n",
    "        else:\n",
    "            actual_date.append(hotspot_date[k])\n",
    "    for j in range(date_length - 3, -1, -1):\n",
    "        final_date.append(actual_date[j])\n",
    "\n",
    "    return (join_string.join(final_date) == climate_date)\n",
    "\n",
    "def get_prec_number(a_string):\n",
    "    # this function gives the float part of a precipitation measurement\n",
    "    final = a_string.strip()\n",
    "    return float(final[0:4])\n",
    "    \n",
    "\n",
    "def get_prec_letter(a_string):\n",
    "    # this function gives the letter representing the tyoe of precipitation measurement\n",
    "    final = a_string.strip()\n",
    "    return final[-1]\n",
    "\n",
    "def change_format(a_date):\n",
    "    date_final = datetime.strptime(a_date, '%d/%m/%y')\n",
    "    formatted = date_final.strftime(\"20%y/%m/%d\")\n",
    "    return formatted\n",
    "\n",
    "# Task 2\n",
    "# Question 1\n",
    "# So we have to load all the data from the csv files into a new database fit3182_assignment_db\n",
    "# Check if the main collection already exists\n",
    "try:\n",
    "    a_collection.drop()\n",
    "except NameError:\n",
    "    pass\n",
    "# Now create the data base\n",
    "client = MongoClient() # so we connect on the default host and port\n",
    "db = client.fit3182_assignment_db # create a new database fit3182_assignment_db\n",
    "a_collection = db.climatee # create a new collection called 'the_collection'\n",
    "with open('climate_historic.csv', 'r') as file_one:  # open climate historic file\n",
    "    # now create a csv object for each of the files\n",
    "    climate_data = csv.reader(file_one)\n",
    "    climate_lines = []\n",
    "    count = 0\n",
    "    for c_line in climate_data:\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        else:\n",
    "            climate_lines.append(c_line)   # put each line of the data in a list \n",
    "            \n",
    "\n",
    "            \n",
    "with open('hotspot_historic.csv', 'r') as file_two: # open hotspot historic file\n",
    "    hotspot_data = csv.reader(file_two)\n",
    "    hotspot_lines = []\n",
    "    count = 0\n",
    "    for h_line in hotspot_data:  # go through the hotspot data.\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        else:\n",
    "            hotspot_lines.append(h_line)  # put each line of the data in a list\n",
    "\n",
    "documents = []\n",
    "for climate_line in climate_lines:\n",
    "    matched = False\n",
    "    for hotspot_line in hotspot_lines:\n",
    "        if samedate(hotspot_line[4], climate_line[1]):\n",
    "            new_entry = {\n",
    "             \"latitude\" : float(hotspot_line[0]),\n",
    "             \"longitude\": float(hotspot_line[1]),\n",
    "             \"datetime\" : hotspot_line[2],\n",
    "             \"confidence\" : float(hotspot_line[3]),\n",
    "             \"surface_temperature_celsius\" : float(hotspot_line[5]),\n",
    "            \"station\": float(climate_line[0]),\n",
    "            \"date\": change_format(climate_line[1]),\n",
    "            \"air_temperature_celsius\" : float(climate_line[2]),\n",
    "            \"relative_humidity\": float(climate_line[3]),\n",
    "            \"windspeed_knots\" : float(climate_line[4]),\n",
    "            \"max_wind_speed\": float(climate_line[5]),\n",
    "            \"precipitation_level\": get_prec_number(climate_line[6]),\n",
    "            \"precipitation_type\": get_prec_letter(climate_line[6]),\n",
    "            \"GHI_w/m2\": float(climate_line[7])}\n",
    "            documents.append(new_entry)\n",
    "            matched = True\n",
    "    if matched == False: # there was not a fire on this date\n",
    "        # Set None's for all the unmatchable values cause there was no fire on this date\n",
    "        no_fire = {\n",
    "            \"latitude\" : None,\n",
    "            \"longitude\": None,\n",
    "            \"datetime\" : None,\n",
    "            \"confidence\" : None,\n",
    "            \"surface_temperature_celsius\": None,\n",
    "            \"station\": float(climate_line[0]),\n",
    "            \"date\": change_format(climate_line[1]),\n",
    "            \"air_temperature_celsius\" : float(climate_line[2]),\n",
    "            \"relative_humidity\": float(climate_line[3]),\n",
    "            \"windspeed_knots\" : float(climate_line[4]),\n",
    "            \"max_wind_speed\": float(climate_line[5]),\n",
    "            \"precipitation_level\": get_prec_number(climate_line[6]),\n",
    "            \"precipitation_type\": get_prec_letter(climate_line[6]),\n",
    "            \"GHI_w/m2\": float(climate_line[7])\n",
    "        }\n",
    "        documents.append(no_fire)\n",
    "\n",
    "\n",
    "result = a_collection.insert_many(documents)  \n",
    "# cursor = a_collection.find({})\n",
    "# for item in cursor:\n",
    "#     pprint(item)\n",
    "# find the latest date in the stream\n",
    "\n",
    "latest_date = a_collection.find().sort(\"date\", pymongo.DESCENDING).limit(1)\n",
    "for k in latest_date:\n",
    "    new_date = datetime.strptime(k['date'], '%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n",
      "Message published successfully. Data:  ['-36.4944', '141.8992', '58', '40', '2019/01/16', 'Producer3']\n",
      "Message published successfully. Data:  ['-34.2708', '141.6158', '79', '52', '2019/01/16', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.273', '146.155', '77', '36', '2019/01/16', 'Producer3']\n",
      "Message published successfully. Data:  ['-37.4047', '143.6615', '69', '45', '2019/01/16', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.388', '141.8309', '93', '73', '2019/01/16', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.943', '143.286', '51', '29', '2019/01/17', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.6664', '144.7828', '82', '55', '2019/01/17', 'Producer3']\n",
      "Message published successfully. Data:  ['-37.327', '148.083', '100', '47', '2019/01/17', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.4149', '141.0156', '85', '60', '2019/01/17', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.9034', '141.0013', '100', '95', '2019/01/17', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.99', '142.9147', '76', '49', '2019/01/18', 'Producer3']\n",
      "Message published successfully. Data:  ['-37.5812', '142.7374', '94', '75', '2019/01/18', 'Producer3']\n",
      "Message published successfully. Data:  ['-34.3914', '141.726', '78', '51', '2019/01/18', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.934', '142.5759', '64', '47', '2019/01/18', 'Producer3']\n",
      "Message published successfully. Data:  ['-37.7236', '142.947', '94', '73', '2019/01/18', 'Producer3']\n",
      "Message published successfully. Data:  ['-35.963', '141.078', '86', '60', '2019/01/19', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.9285', '143.9622', '66', '43', '2019/01/19', 'Producer3']\n",
      "Message published successfully. Data:  ['-35.0691', '141.4546', '80', '56', '2019/01/19', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.6828', '144.784', '90', '66', '2019/01/19', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.0663', '145.6855', '87', '62', '2019/01/19', 'Producer3']\n",
      "Message published successfully. Data:  ['-37.8475', '147.2512', '76', '50', '2019/01/20', 'Producer3']\n",
      "Message published successfully. Data:  ['-37.3863', '142.8822', '85', '59', '2019/01/20', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.282', '146.157', '100', '71', '2019/01/20', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.2726', '145.506', '77', '50', '2019/01/20', 'Producer3']\n",
      "Message published successfully. Data:  ['-38.0326', '141.5413', '83', '56', '2019/01/20', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.779', '146.108', '61', '32', '2019/01/21', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.9887', '142.6515', '82', '55', '2019/01/21', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.4384', '141.7444', '81', '55', '2019/01/21', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.7199', '141.3946', '82', '55', '2019/01/21', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.4133', '141.0286', '52', '47', '2019/01/21', 'Producer3']\n",
      "Message published successfully. Data:  ['-37.3226', '143.5442', '78', '51', '2019/01/22', 'Producer3']\n",
      "Message published successfully. Data:  ['-35.708', '143.7987', '50', '38', '2019/01/22', 'Producer3']\n",
      "Message published successfully. Data:  ['-36.4936', '143.9868', '72', '46', '2019/01/22', 'Producer3']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8f3d097372c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mpublish_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parsed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mindex_of_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('hotspot_TERRA_streaming.csv', 'r') as file_four: # open hotspot historic file\n",
    "    terra_data = csv.reader(file_four)\n",
    "    terra_lines = []\n",
    "    count = 0\n",
    "    for line in terra_data:  # go through the hotspot data.\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        else:\n",
    "            terra_lines.append(line)  # put each line of the data in a list\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully. Data: ', data)\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    topic = 'Producer3'\n",
    "    \n",
    "    print('Publishing records..')\n",
    "    producer = connect_kafka_producer()\n",
    "    length_lines = len(terra_lines)\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        if time.time() - start_time <= 10:\n",
    "            index_of_line = random.randrange(0, length_lines)\n",
    "            new_date = new_date\n",
    "            str_date = new_date.strftime('%Y/%m/%d')\n",
    "            line_sent = terra_lines[index_of_line] + [str_date] + [topic]\n",
    "            data = str(line_sent)\n",
    "            publish_message(producer, topic, 'parsed', data)\n",
    "            sleep(2)\n",
    "        else:\n",
    "            index_of_line = random.randrange(0, length_lines)\n",
    "            new_date = new_date + timedelta(days = 1)\n",
    "            str_date = new_date.strftime('%Y/%m/%d')\n",
    "            line_sent = terra_lines[index_of_line] + [str_date] + [topic]\n",
    "            data = str(line_sent)\n",
    "            publish_message(producer, topic, 'parsed', data)\n",
    "            start_time = time.time()\n",
    "            sleep(2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
